{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0afb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix, classification_report\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Credit Card Fraud Detection System with Naive Bayes Algorithm\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc637f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Dataset from Online Sources\n",
    "print(\"\\n1. Loading Dataset from Online Sources...\")\n",
    "\n",
    "try:\n",
    "    # Direct download link from Kaggle\n",
    "    url = \"https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/download?datasetVersionNumber=3\"\n",
    "    \n",
    "    # Alternative reliable URL\n",
    "    alt_url = \"https://raw.githubusercontent.com/nsethi31/Kaggle-Data-Credit-Card-Fraud-Detection/master/creditcard.csv\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(url)\n",
    "        print(\"✓ Dataset successfully loaded from Kaggle URL\")\n",
    "    except:\n",
    "        df = pd.read_csv(alt_url)\n",
    "        print(\"✓ Dataset loaded from alternative GitHub URL\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading from online sources: {e}\")\n",
    "    print(\"Creating synthetic dataset for demonstration...\")\n",
    "    np.random.seed(42)\n",
    "    n_samples = 10000\n",
    "    n_fraud = int(n_samples * 0.0017)  # Real fraud ratio\n",
    "    \n",
    "    # Create synthetic data similar to original dataset\n",
    "    data = {}\n",
    "    for i in range(28):\n",
    "        data[f'V{i+1}'] = np.random.normal(0, 1, n_samples)\n",
    "    \n",
    "    data['Time'] = np.random.uniform(0, 172000, n_samples)\n",
    "    data['Amount'] = np.random.exponential(100, n_samples)\n",
    "    \n",
    "    # Generate labels\n",
    "    labels = np.zeros(n_samples)\n",
    "    fraud_indices = np.random.choice(n_samples, n_fraud, replace=False)\n",
    "    labels[fraud_indices] = 1\n",
    "    data['Class'] = labels\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"✓ Synthetic dataset created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb01b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Comprehensive Dataset Report\n",
    "print(\"\\n2. Comprehensive Dataset Report:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Dataset dimensions: {df.shape}\")\n",
    "print(f\"Number of samples: {df.shape[0]:,}\")\n",
    "print(f\"Number of features: {df.shape[1]}\")\n",
    "print(f\"Number of normal transactions: {len(df[df['Class'] == 0]):,}\")\n",
    "print(f\"Number of fraudulent transactions: {len(df[df['Class'] == 1]):,}\")\n",
    "\n",
    "# Class distribution analysis\n",
    "class_distribution = df['Class'].value_counts()\n",
    "fraud_ratio = class_distribution[1] / len(df) * 100\n",
    "print(f\"\\nFraud ratio in dataset: {fraud_ratio:.4f}%\")\n",
    "\n",
    "print(\"\\nDataset information:\")\n",
    "print(df.info())\n",
    "print(\"\\nDescriptive statistics of numerical features:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nChecking for missing values:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"✓ No missing values found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc5fbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Exploratory Data Analysis (EDA) - ONLY CLASS DISTRIBUTION CHART\n",
    "print(\"\\n3. Exploratory Data Analysis...\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Chart 1: Class distribution - ONLY THIS CHART\n",
    "class_counts = df['Class'].value_counts()\n",
    "plt.pie(class_counts.values, labels=['Normal Transaction', 'Fraudulent Transaction'], \n",
    "        autopct='%1.3f%%', colors=['lightgreen', 'red'], startangle=90)\n",
    "plt.title('Class Distribution in Dataset\\n(Highly Imbalanced)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1ec431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Data Preprocessing\n",
    "print(\"\\n4. Data Preprocessing...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check and remove duplicate values\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate samples: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"✓ Duplicate samples removed\")\n",
    "\n",
    "# Standardize Time and Amount features\n",
    "df_processed = df.copy()\n",
    "scaler = RobustScaler()\n",
    "df_processed[['Time', 'Amount']] = scaler.fit_transform(df_processed[['Time', 'Amount']])\n",
    "\n",
    "print(\"✓ Time and Amount features standardized\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = df_processed.drop('Class', axis=1)\n",
    "y = df_processed['Class']\n",
    "\n",
    "print(f\"X dimensions after preprocessing: {X.shape}\")\n",
    "print(f\"y dimensions after preprocessing: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f220ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Handling Data Imbalance with Under-Sampling\n",
    "print(\"\\n5. Handling Data Imbalance...\")\n",
    "\n",
    "print(f\"Class distribution before sampling: {Counter(y)}\")\n",
    "\n",
    "# Use Under-Sampling to create balanced data\n",
    "rus = RandomUnderSampler(random_state=42, sampling_strategy=0.5)\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "print(f\"Class distribution after sampling: {Counter(y_resampled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ba4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Split Data into Train and Test Sets\n",
    "print(\"\\n6. Splitting Data...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.3, random_state=42, stratify=y_resampled\n",
    ")\n",
    "\n",
    "print(f\"Training data dimensions: {X_train.shape}\")\n",
    "print(f\"Testing data dimensions: {X_test.shape}\")\n",
    "print(f\"Class distribution in training data: {Counter(y_train)}\")\n",
    "print(f\"Class distribution in testing data: {Counter(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1b5bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Data Standardization\n",
    "print(\"\\n7. Standardizing Data...\")\n",
    "scaler_final = StandardScaler()\n",
    "X_train_scaled = scaler_final.fit_transform(X_train)\n",
    "X_test_scaled = scaler_final.transform(X_test)\n",
    "\n",
    "print(\"✓ Data standardized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d3c7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Train Naive Bayes Model\n",
    "print(\"\\n8. Training Naive Bayes Model...\")\n",
    "model = GaussianNB()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "print(\"✓ Model trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9da246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Model Prediction and Evaluation\n",
    "print(\"\\n9. Model Evaluation...\")\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_proba = model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "# Calculate F1-Score\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb6e58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Confusion Matrix\n",
    "print(\"\\n10. Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902b601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Complete Classification Report\n",
    "print(\"\\n11. Complete Classification Report:\")\n",
    "print(\"-\" * 50)\n",
    "print(classification_report(y_test, y_pred, \n",
    "                          target_names=['Normal Transaction', 'Fraudulent Transaction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e7f97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Model Performance Analysis on Original Data\n",
    "print(\"\\n12. Evaluation on Original Data (Real Test)...\")\n",
    "# Use original data for final test\n",
    "X_original = df_processed.drop('Class', axis=1)\n",
    "y_original = df_processed['Class']\n",
    "\n",
    "# Split original data\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "    X_original, y_original, test_size=0.3, random_state=42, stratify=y_original\n",
    ")\n",
    "\n",
    "# Standardize and predict\n",
    "X_test_orig_scaled = scaler_final.transform(X_test_orig)\n",
    "y_pred_orig = model.predict(X_test_orig_scaled)\n",
    "\n",
    "# Calculate metrics for original data\n",
    "cm_orig = confusion_matrix(y_test_orig, y_pred_orig)\n",
    "recall_orig = recall_score(y_test_orig, y_pred_orig)\n",
    "precision_orig = precision_score(y_test_orig, y_pred_orig)\n",
    "\n",
    "print(f\"Recall on original data: {recall_orig:.4f}\")\n",
    "print(f\"Precision on original data: {precision_orig:.4f}\")\n",
    "print(\"Confusion matrix on original data:\")\n",
    "print(cm_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a5a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Final Conclusion\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Final Conclusion and Recommendations\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n Naive Bayes Model Performance:\")\n",
    "print(f\"• Overall Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"• Recall: {recall*100:.2f}% (Ability to detect fraudulent transactions)\")\n",
    "print(f\"• Precision: {precision*100:.2f}% (Accuracy in detecting fraud)\")\n",
    "print(f\"• F1-Score: {f1*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n Confusion Matrix Analysis:\")\n",
    "print(f\"• True Negative (TN): {cm[0,0]:,} - Normal transactions correctly classified\")\n",
    "print(f\"• False Positive (FP): {cm[0,1]:,} - Normal transactions classified as fraud\")\n",
    "print(f\"• False Negative (FN): {cm[1,0]:,} - Fraud transactions classified as normal\")\n",
    "print(f\"• True Positive (TP): {cm[1,1]:,} - Fraud transactions correctly classified\")\n",
    "\n",
    "print(f\"\\n Important Notes:\")\n",
    "print(f\"• Fraud ratio in original dataset: {fraud_ratio:.4f}% (Highly Imbalanced)\")\n",
    "print(f\"• Under-Sampling used to handle class imbalance\")\n",
    "print(f\"• High Recall ({recall*100:.2f}%) is crucial for fraud detection\")\n",
    "print(f\"• Model needs threshold tuning for real-world deployment\")\n",
    "\n",
    "print(f\"\\n Recommendations for Improvement:\")\n",
    "print(f\"1. Use advanced sampling techniques (SMOTE)\")\n",
    "print(f\"2. Adjust classification threshold to optimize Recall/Precision\")\n",
    "print(f\"3. Try other algorithms like Isolation Forest or XGBoost\")\n",
    "print(f\"4. Use Cross-Validation for more robust evaluation\")\n",
    "\n",
    "print(f\"\\n Model Training Note:\")\n",
    "print(\"• Model is trained fresh each time (no .pkl file saved)\")\n",
    "print(\"• For production use, consider implementing model persistence\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Model Training and Evaluation Completed Successfully!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
